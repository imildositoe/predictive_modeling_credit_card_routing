{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Credit Card Routing System__\n",
    "\n",
    "This is the main notebook of the project and is divided by 2 files (this and app.py). This file is subdivided by:\n",
    "- Main Imports: import all the libraries needed for the project\n",
    "- Read Data: read the data from the exce sheet dataset\n",
    "- Exploratory Data Analysis: explore the data for consequent analysis and development of the model\n",
    "- Data Preparation\n",
    "- Modeling\n",
    "- Evaluation\n",
    "- End-user GUI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "import joblib\n",
    "import streamlit as st\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These lines are responsible for loading the Excel dataset and printing the head\n",
    "df = pd.read_excel('data/PSP_Jan_Feb_2019.xlsx')\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Histogram of Distribution of Transactions__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This snippet is for plotting the histogram of transaction amounts\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df['amount'], bins=100, kde=True, color='#e6b8af')\n",
    "plt.title('Distribution of Amounts of Transactions')\n",
    "plt.grid(True)\n",
    "plt.xlabel('Amounts of Transactions')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Bar Chart with the Success Rate (by 3D-Secure Status)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This snippet is for calculating the success rate by 3D-secure status\n",
    "success_rates = df.groupby('3D_secured')['success'].mean().reset_index()\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.barplot(x='3D_secured', y='success', data=success_rates, palette='coolwarm')\n",
    "plt.title('Success Rate (by 3D-Secure Status)')\n",
    "plt.xlabel('0 = Has 3D-Secure, 1 = Does not Have 3D-Secure')\n",
    "plt.ylabel('Success Rate')\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Bar Chart with the Transaction Success Rates (by PSP)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Line to calculate the success rates per each PSP\n",
    "psp_success_rates = df.groupby('PSP')['success'].mean().reset_index()\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.barplot(x='PSP', y='success', data=psp_success_rates, palette='viridis')\n",
    "plt.title('Transaction Success Rate (by PSP)')\n",
    "plt.xlabel('Payment Service Provider')\n",
    "plt.ylabel('Success Rate')\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Retry Patterns__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snippet to identify retry patterns\n",
    "# Transactions within 1 minute with the same country & amount\n",
    "df_exploration = df\n",
    "df_exploration['tmsp'] = pd.to_datetime(df_exploration['tmsp'])\n",
    "\n",
    "df_exploration = df_exploration.sort_values(by=['country', 'amount', 'tmsp'])\n",
    "\n",
    "df_exploration['retry'] = (df_exploration.groupby(['country', 'amount'])['tmsp'].diff().dt.total_seconds() <= 60).astype(int)\n",
    "\n",
    "palette = {0: 'green', 1: 'pink'}\n",
    "plt.figure(figsize=(10, 5))\n",
    "scatter = sns.scatterplot(x=df_exploration.index, y=df_exploration['amount'], hue=df_exploration['retry'], data=df_exploration, palette=palette, alpha=0.6)\n",
    "\n",
    "handles, labels = scatter.get_legend_handles_labels()\n",
    "plt.legend(handles=handles, labels=[\"No Retry\", \"Retry\"], title=\"Retry Status\")\n",
    "\n",
    "plt.title('Patterns of Transaction Retries')\n",
    "plt.xlabel('Index of the Transaction')\n",
    "plt.ylabel('Transaction Amount (euros)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Data Cleaning__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "\n",
    "# Snippet to convert the timestamp into a datetime format and to fill missing categorical values with mode\n",
    "df['tmsp'] = pd.to_datetime(df['tmsp'])\n",
    "for col in ['country', 'PSP', '3D_secured', 'card']:\n",
    "    df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "\n",
    "# Line to fill missing values with median\n",
    "df['amount'].fillna(df['amount'].median(), inplace=True)\n",
    "\n",
    "# Drop duplicate transactions (keeping first occurrence)\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Lines to display updated data\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Feature Engineering__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snippet to identify retry patterns\n",
    "# Transactions within 1 minute with the same country & amount\n",
    "df = df.sort_values(by=['country', 'amount', 'tmsp'])\n",
    "df['retry'] = (df.groupby(['country', 'amount'])['tmsp'].diff().dt.total_seconds() <= 60).astype(int)\n",
    "\n",
    "# Snippet to create hour, day of the week, and month extracted from the timestamp\n",
    "df['hour'] = df['tmsp'].dt.hour\n",
    "df['day_of_week'] = df['tmsp'].dt.dayofweek\n",
    "df['month'] = df['tmsp'].dt.month\n",
    "\n",
    "df['3D_secured'] = df['3D_secured'].astype(int)\n",
    "\n",
    "# Encode PSPs and defining transaction fees\n",
    "psp_mapping = {\"Moneycard\": 1, \"Goldcard\": 2, \"UK_Card\": 3, \"Simplecard\": 4}\n",
    "df['PSP_code'] = df['PSP'].map(psp_mapping)\n",
    "\n",
    "psp_encoder = LabelEncoder()\n",
    "df['PSP_code'] = psp_encoder.fit_transform(df['PSP_code'])\n",
    "joblib.dump(psp_encoder, \"label_encoder_psp.pkl\")\n",
    "\n",
    "psp_fees = {\n",
    "    \"Moneycard\": {1: 7.5, 0: 4.5},\n",
    "    \"Goldcard\": {1: 12.5, 0: 7.5},\n",
    "    \"UK_Card\": {1: 5.5, 0: 1.5},\n",
    "    \"Simplecard\": {1: 1.5, 0: 1}\n",
    "}\n",
    "\n",
    "df['transaction_cost'] = df.apply(lambda x: psp_fees[x['PSP']][x['success']], axis=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Outlier Treatment__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treat the outliers by limitting at percentile 99th\n",
    "upper_limit = df['amount'].quantile(0.99)\n",
    "df['amount'] = np.where(df['amount'] > upper_limit, upper_limit, df['amount'])\n",
    "\n",
    "sns.boxplot(x=df['amount'], color='red')\n",
    "plt.title(\"Transaction Amount Distribution (After Outlier Treatment)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Data Preprocessing__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = ['amount', '3D_secured', 'retry', 'hour', 'day_of_week', 'month', 'PSP_code']\n",
    "\n",
    "target_col = 'success'\n",
    "\n",
    "# Snippet to encode the attributes and save them into pkl files\n",
    "label_encoder_card = LabelEncoder()\n",
    "label_encoder_country = LabelEncoder()\n",
    "df['card'] = label_encoder_card.fit_transform(df['card'])\n",
    "df['country'] = label_encoder_country.fit_transform(df['country'])\n",
    "\n",
    "joblib.dump(label_encoder_card, \"label_encoder_card.pkl\")\n",
    "joblib.dump(label_encoder_country, \"label_encoder_country.pkl\")\n",
    "\n",
    "print(\"Encoders saved successfully!\")\n",
    "\n",
    "X = df[feature_cols]\n",
    "y = df[target_col]\n",
    "\n",
    "# Creating a split of 80% train and 20% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_test_original = X_test.copy()\n",
    "\n",
    "# Scalling the features and saving them into a pkl file jointly with the scaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "joblib.dump(df[feature_cols].columns.tolist(), \"expected_features.pkl\")\n",
    "joblib.dump(scaler, \"scaler.pkl\")\n",
    "print(\"Scaler saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Baseline Model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snippet for the baseline classifier\n",
    "most_common_class = y_train.value_counts().idxmax()\n",
    "y_pred_baseline = np.full(y_test.shape, most_common_class)\n",
    "\n",
    "print(\"Baseline Performance:\")\n",
    "print(classification_report(y_test, y_pred_baseline))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Logistic Regression__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snippet for the logistic classifier\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "y_pred_log_reg = log_reg.predict(X_test)\n",
    "\n",
    "print(\"Logistic Regression Performance:\")\n",
    "print(classification_report(y_test, y_pred_log_reg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Random Forest Classifier__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snippet for the random forest classifier\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "print(\"Random Forest Performance:\")\n",
    "print(classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__XGBoost Classifier__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snippet for the xgboost classifier\n",
    "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "y_pred_xgb = xgb.predict(X_test)\n",
    "\n",
    "print(\"XGBoost Performance:\")\n",
    "print(classification_report(y_test, y_pred_xgb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning, Evaluation, and Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Hyperparameter Tuning for All Models__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the parameters of all models\n",
    "logistic_params = {\n",
    "    'C': [0.01, 0.1, 1, 10], \n",
    "    'solver': ['liblinear', 'lbfgs']\n",
    "}\n",
    "\n",
    "rf_params = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "xgb_params = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "\n",
    "# Baseline Model\n",
    "y_pred_baseline = [1] * len(y_test)\n",
    "print(\"Baseline Model Performance:\")\n",
    "print(classification_report(y_test, y_pred_baseline))\n",
    "\n",
    "# Logistic Regression Model\n",
    "logistic_grid = GridSearchCV(LogisticRegression(random_state=42), logistic_params, cv=5, scoring='f1')\n",
    "logistic_grid.fit(X_train, y_train)\n",
    "best_logistic = logistic_grid.best_estimator_\n",
    "y_pred_logistic = best_logistic.predict(X_test)\n",
    "\n",
    "# Random Forest Model\n",
    "rf_grid = GridSearchCV(RandomForestClassifier(random_state=42), rf_params, cv=5, scoring='f1')\n",
    "rf_grid.fit(X_train, y_train)\n",
    "best_rf = rf_grid.best_estimator_\n",
    "y_pred_rf = best_rf.predict(X_test)\n",
    "\n",
    "# XGBoost Model\n",
    "xgb_grid = GridSearchCV(XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42), \n",
    "                         xgb_params, cv=5, scoring='f1')\n",
    "xgb_grid.fit(X_train, y_train)\n",
    "best_xgb = xgb_grid.best_estimator_\n",
    "y_pred_xgb = best_xgb.predict(X_test)\n",
    "\n",
    "# Evaluation of the models\n",
    "models = {\n",
    "    \"Baseline\": y_pred_baseline,\n",
    "    \"Logistic Regression\": y_pred_logistic,\n",
    "    \"Random Forest\": y_pred_rf,\n",
    "    \"XGBoost\": y_pred_xgb\n",
    "}\n",
    "\n",
    "for model_name, y_pred in models.items():\n",
    "    print(f\"\\n{model_name} Model Performance:\")\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Visual Comparison of Model Performances__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell to plot a grouped bar chart to compare model performances\n",
    "models = {\n",
    "    \"Baseline\": y_pred_baseline,\n",
    "    \"Logistic Regression\": y_pred_logistic,\n",
    "    \"Random Forest\": y_pred_rf,\n",
    "    \"XGBoost\": y_pred_xgb\n",
    "}\n",
    "\n",
    "model_performance = {}\n",
    "\n",
    "# Calculation of metrics of the models\n",
    "for model_name, y_pred in models.items():\n",
    "    model_performance[model_name] = {\n",
    "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"Precision\": precision_score(y_test, y_pred, zero_division=0),\n",
    "        \"Recall\": recall_score(y_test, y_pred, zero_division=0),\n",
    "        \"F1-score\": f1_score(y_test, y_pred, zero_division=0)\n",
    "    }\n",
    "\n",
    "performance_df = pd.DataFrame(model_performance).T\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "performance_df.plot(kind=\"bar\", colormap=\"brg\", figsize=(12, 6), edgecolor=\"black\")\n",
    "plt.title(\"Comparison of Model Performances\", fontsize=14)\n",
    "plt.ylabel(\"Score\", fontsize=12)\n",
    "plt.xlabel(\"Models\", fontsize=12)\n",
    "plt.xticks(rotation=0)\n",
    "plt.legend(title=\"METRICS\")\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Confusion Matrix for All Models__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "model_names = [\"Baseline\", \"Logistic Regression\", \"Random Forest\", \"XGBoost\"]\n",
    "y_preds = [y_pred_baseline, y_pred_logistic, y_pred_rf, y_pred_xgb]\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    sns.heatmap(confusion_matrix(y_test, y_preds[i]), annot=True, fmt='d', cmap=\"Greens\", cbar=True, ax=ax)\n",
    "    ax.set_title(f\"{model_names[i]}\")\n",
    "    ax.set_xlabel(\"Predicted\")\n",
    "    ax.set_ylabel(\"Actual\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Comparing Model Scores and Costs of Each PSP and Selecting the Best Model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing the transaction fees per each PSP and transaction status\n",
    "psp_costs = {\n",
    "    1: {\"success\": 7.5, \"failure\": 4.5},\n",
    "    2: {\"success\": 12.5, \"failure\": 7.5},\n",
    "    3: {\"success\": 5.5, \"failure\": 1.5},\n",
    "    4: {\"success\": 1.5, \"failure\": 1}\n",
    "}\n",
    "\n",
    "psp_mapping = {\"Moneycard\": 1, \"Goldcard\": 2, \"UK_Card\": 3, \"Simplecard\": 4}\n",
    "\n",
    "# Function to calculate the total cost of each transaction\n",
    "def calculate_total_cost(y_true, y_pred, psp_costs, psp_column):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    TN, FP, FN, TP = cm.ravel()\n",
    "    \n",
    "    success_costs = np.where(y_pred == 1, psp_column.map(lambda x: psp_costs[x][\"success\"]), 0)\n",
    "    failure_costs = np.where(y_pred == 0, psp_column.map(lambda x: psp_costs[x][\"failure\"]), 0)\n",
    "\n",
    "    total_cost = np.sum(success_costs) + np.sum(failure_costs)\n",
    "    return total_cost\n",
    "\n",
    "\n",
    "# Snippet to extract the cost and compute the final performance-cost\n",
    "model_costs = {\n",
    "    \"Baseline\": calculate_total_cost(y_test, y_pred_baseline, psp_costs, X_test_original[\"PSP_code\"]),\n",
    "    \"Logistic Regression\": calculate_total_cost(y_test, y_pred_logistic, psp_costs, X_test_original[\"PSP_code\"]),\n",
    "    \"Random Forest\": calculate_total_cost(y_test, y_pred_rf, psp_costs, X_test_original[\"PSP_code\"]),\n",
    "    \"XGBoost\": calculate_total_cost(y_test, y_pred_xgb, psp_costs, X_test_original[\"PSP_code\"])\n",
    "}\n",
    "\n",
    "model_f1_scores = {\n",
    "    \"Logistic Regression\": f1_score(y_test, y_pred_logistic),\n",
    "    \"Random Forest\": f1_score(y_test, y_pred_rf),\n",
    "    \"XGBoost\": f1_score(y_test, y_pred_xgb)\n",
    "}\n",
    "\n",
    "max_cost = max(model_costs.values())\n",
    "normalized_costs = {model: max_cost / cost for model, cost in model_costs.items()}\n",
    "\n",
    "final_scores = {model: model_f1_scores[model] * normalized_costs[model] for model in model_f1_scores}\n",
    "\n",
    "for model in model_f1_scores.keys():\n",
    "    print(f\"{model}:\")\n",
    "    print(f\"   - F1-Score: {model_f1_scores[model]:.4f}\")\n",
    "    print(f\"   - Total Cost: {model_costs[model]:.2f} Euros\")\n",
    "    print(f\"   - Performance-Cost Score: {final_scores[model]:.4f}\")\n",
    "\n",
    "# Snippet to select the best performant model\n",
    "best_model = max(final_scores, key=final_scores.get)\n",
    "print(f\"\\nBest Performant Model: {best_model}\")\n",
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": logistic_grid.best_estimator_,\n",
    "    \"Random Forest\": rf_grid.best_estimator_,\n",
    "    \"XGBoost\": xgb_grid.best_estimator_\n",
    "}\n",
    "\n",
    "best_model_object = models[best_model]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Model Deployment__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell to save the best chosen model into a pkl file\n",
    "joblib.dump(best_model_object, \"best_model.pkl\")\n",
    "print(\"The model has been saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Prediction Pipeline__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a prediction pipeline to avoid re-training for each transaction\n",
    "class PredictionPipeline:\n",
    "    def __init__(self, model_path, encoder_card_path, encoder_country_path, scaler_path):\n",
    "        self.model = joblib.load(model_path) \n",
    "        self.encoder_card = joblib.load(encoder_card_path)\n",
    "        self.encoder_country = joblib.load(encoder_country_path)\n",
    "        self.scaler = joblib.load(scaler_path)\n",
    "\n",
    "    def preprocess(self, df):\n",
    "        df['card'] = self.encoder_card.transform(df['card'])\n",
    "        df['country'] = self.encoder_country.transform(df['country'])\n",
    "\n",
    "        scaler = joblib.load(\"scaler.pkl\")\n",
    "        df_scaled = scaler.transform(df)\n",
    "        return df_scaled\n",
    "\n",
    "    def predict(self, df):\n",
    "        processed_data = self.preprocess(df)\n",
    "        return self.model.predict(processed_data)\n",
    "\n",
    "# Save the pipeline\n",
    "joblib.dump(PredictionPipeline(\"best_model.pkl\", \"label_encoder_card.pkl\", \"label_encoder_country.pkl\", \"scaler.pkl\"), \"prediction_pipeline.pkl\")\n",
    "\n",
    "print(\"The prediction pipelines have been saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__End-user GUI__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The end-user GUI code is written in the file \"app.py\" to allow the GUI to executed via terminal and be opened via browser."
   ]
  }
